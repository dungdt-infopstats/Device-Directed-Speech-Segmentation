{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Phase3_ForceAlighment",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10udCryp7/TV-command-synthesis/blob/main/src_prototype/Phase3_ForceAlighment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1MrkteyPMIhLgki82_qJ_A9JDOIme3za3\n",
        "!unzip -q 100-samples.zip"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:04:50.8523Z",
          "iopub.execute_input": "2025-08-18T04:04:50.85256Z",
          "iopub.status.idle": "2025-08-18T04:04:56.249757Z",
          "shell.execute_reply.started": "2025-08-18T04:04:50.852538Z",
          "shell.execute_reply": "2025-08-18T04:04:56.249005Z"
        },
        "id": "Ng6dr1iRcdDu",
        "outputId": "82b1532e-6243-4343-f4fa-17eda9ec364f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading...\nFrom (original): https://drive.google.com/uc?id=1MrkteyPMIhLgki82_qJ_A9JDOIme3za3\nFrom (redirected): https://drive.google.com/uc?id=1MrkteyPMIhLgki82_qJ_A9JDOIme3za3&confirm=t&uuid=9bf2c4df-5c83-4cb1-ba36-17a2d2df8a57\nTo: /kaggle/working/100-samples.zip\n100%|███████████████████████████████████████| 52.8M/52.8M [00:00<00:00, 224MB/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 121dcbCvDVzB22uelCeYkg4Fs8TVbQ1tp"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:04:56.251475Z",
          "iopub.execute_input": "2025-08-18T04:04:56.251689Z",
          "iopub.status.idle": "2025-08-18T04:04:59.040713Z",
          "shell.execute_reply.started": "2025-08-18T04:04:56.251669Z",
          "shell.execute_reply": "2025-08-18T04:04:59.040031Z"
        },
        "id": "u35Ul_iHcdDv",
        "outputId": "ecb8b0b7-1cba-4c09-d485-38cf901eb152"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading...\nFrom: https://drive.google.com/uc?id=121dcbCvDVzB22uelCeYkg4Fs8TVbQ1tp\nTo: /kaggle/working/100-samples.csv\n100%|██████████████████████████████████████| 29.0k/29.0k [00:00<00:00, 56.8MB/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"100-samples.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:04:59.041721Z",
          "iopub.execute_input": "2025-08-18T04:04:59.041959Z",
          "iopub.status.idle": "2025-08-18T04:04:59.322734Z",
          "shell.execute_reply.started": "2025-08-18T04:04:59.041937Z",
          "shell.execute_reply": "2025-08-18T04:04:59.321966Z"
        },
        "id": "PVqQacgRcdDv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U openai-whisper\n",
        "!pip install -q -U faster-whisper"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:04:59.323665Z",
          "iopub.execute_input": "2025-08-18T04:04:59.323909Z",
          "iopub.status.idle": "2025-08-18T04:06:25.833715Z",
          "shell.execute_reply.started": "2025-08-18T04:04:59.323892Z",
          "shell.execute_reply": "2025-08-18T04:06:25.833036Z"
        },
        "id": "4AV3FYPTcdDw",
        "outputId": "1a2099d6-e089-4911-ecbd-97807dbd06b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import itertools\n",
        "from faster_whisper import WhisperModel\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "import ast\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "from multiprocessing import cpu_count\n",
        "import threading\n",
        "from functools import lru_cache\n",
        "import torch\n",
        "import gc\n",
        "from typing import Optional, Tuple, List, Dict, Any\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class OptimizedSpeechCleaning:\n",
        "    def __init__(self, cant_clean_list=None, max_workers=None, use_gpu=True, model_size=\"medium\"):\n",
        "        \"\"\"\n",
        "        Khởi tạo với các tối ưu:\n",
        "        - faster-whisper với GPU support\n",
        "        - Thread pool cho I/O operations\n",
        "        - Memory management\n",
        "        \"\"\"\n",
        "        # Determine device và compute type\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            # self.compute_type = \"float16\"\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            # self.compute_type = \"int8\"  # Tối ưu cho CPU\n",
        "            print(\"Using CPU\")\n",
        "\n",
        "        # Load faster-whisper model\n",
        "        self.model = WhisperModel(\n",
        "            model_size,\n",
        "            device=self.device,\n",
        "            # compute_type=self.compute_type,\n",
        "            cpu_threads=cpu_count() if self.device == \"cpu\" else 4\n",
        "        )\n",
        "\n",
        "        self.cant_clean_list = cant_clean_list or []\n",
        "        self.max_workers = max_workers or min(cpu_count(), 8)  # Giới hạn threads\n",
        "\n",
        "        # Cache cho text normalization\n",
        "        self._normalize_cache = {}\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "        print(f\"Initialized with {self.max_workers} workers on {self.device}\")\n",
        "\n",
        "    @lru_cache(maxsize=10000)\n",
        "    def normalize_text_cached(self, text: str) -> str:\n",
        "        \"\"\"Cached version của normalize_text\"\"\"\n",
        "        return re.sub(r'[^\\w\\s]', '', text.lower()).strip()\n",
        "\n",
        "    def clean_pipeline(self, export_dir: str, data: pd.DataFrame, speech_folder: str, padding=None, batch_size=4):\n",
        "        \"\"\"\n",
        "        Tối ưu pipeline với:\n",
        "        - Batch processing\n",
        "        - Parallel execution\n",
        "        - Memory management\n",
        "        \"\"\"\n",
        "        os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "        # Chia data thành batches để tránh overload GPU\n",
        "        batches = [data.iloc[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
        "\n",
        "        total_processed = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            print(f\"Processing batch {batch_idx + 1}/{len(batches)}\")\n",
        "\n",
        "            # Process batch với ThreadPoolExecutor\n",
        "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                futures = []\n",
        "\n",
        "                for idx, row in batch.iterrows():\n",
        "                    future = executor.submit(\n",
        "                        self._process_single_file,\n",
        "                        export_dir, row, speech_folder, padding\n",
        "                    )\n",
        "                    futures.append(future)\n",
        "\n",
        "                # Collect results\n",
        "                for future in futures:\n",
        "                    try:\n",
        "                        future.result(timeout=300)  # 5 minutes timeout\n",
        "                        total_processed += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing file: {e}\")\n",
        "\n",
        "            # Memory cleanup after each batch\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Processed {total_processed} files in {end_time - start_time:.2f} seconds\")\n",
        "        print(f\"Average: {(end_time - start_time) / total_processed:.2f} seconds per file\")\n",
        "\n",
        "    def _process_single_file(self, export_dir: str, row: pd.Series, speech_folder: str, padding):\n",
        "        \"\"\"Process một file duy nhất\"\"\"\n",
        "        file_name = f\"{row['type']}_{row['id']}\"\n",
        "\n",
        "        # Process full file\n",
        "        self._process_single_segment(\n",
        "            export_dir, file_name, \"full\", speech_folder, padding, row\n",
        "        )\n",
        "\n",
        "        # Process segments if applicable\n",
        "        if row['type'] in ['single_mix', 'chain_mix']:\n",
        "            segment_tasks = []\n",
        "\n",
        "            # Tạo tasks cho các segments\n",
        "            for i in range(row['num_segments']):\n",
        "                segment_tasks.append((export_dir, file_name, f\"seg_{i}\", speech_folder, padding, row))\n",
        "\n",
        "            # Process segments song song (nested parallelism với ít threads hơn)\n",
        "            with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "                futures = [\n",
        "                    executor.submit(self._process_single_segment, *task)\n",
        "                    for task in segment_tasks\n",
        "                ]\n",
        "\n",
        "                for future in futures:\n",
        "                    try:\n",
        "                        future.result()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing segment: {e}\")\n",
        "\n",
        "    def _process_single_segment(self, export_dir: str, file_name: str, file_type: str,\n",
        "                               speech_folder: str, padding, row: pd.Series):\n",
        "        \"\"\"Process một segment\"\"\"\n",
        "        try:\n",
        "            start, end = self.get_clean_range(\n",
        "                file_name=file_name,\n",
        "                file_type=file_type,\n",
        "                padding=padding,\n",
        "                speech_folder=speech_folder\n",
        "            )\n",
        "\n",
        "            if start is not None and end is not None:\n",
        "                trimmed_audio = self.trim_audio(\n",
        "                    file_name=file_name,\n",
        "                    file_type=file_type,\n",
        "                    start=start,\n",
        "                    end=end,\n",
        "                    speech_folder=speech_folder\n",
        "                )\n",
        "\n",
        "                # Create output directory\n",
        "                output_dir = Path(export_dir) / file_name\n",
        "                output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                out_path = output_dir / f\"{file_name}_{file_type}_trimmed.wav\"\n",
        "                trimmed_audio.export(str(out_path), format=\"wav\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}_{file_type}: {e}\")\n",
        "            with self._lock:\n",
        "                self.cant_clean_list.append((file_name, file_type, str(e)))\n",
        "\n",
        "    def get_clean_range(self, file_name: str, file_type: str, speech_folder: str, padding=None):\n",
        "        \"\"\"Tối ưu transcription với faster-whisper\"\"\"\n",
        "        file_path = os.path.join(speech_folder, file_name, f\"{file_name}_{file_type}.wav\")\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File not found: {file_path}\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            # Sử dụng faster-whisper với các tối ưu\n",
        "            segments, info = self.model.transcribe(\n",
        "                file_path,\n",
        "                word_timestamps=True,\n",
        "                vad_filter=True,  # Voice Activity Detection để tối ưu\n",
        "                vad_parameters=dict(min_silence_duration_ms=500),\n",
        "                beam_size=1,  # Giảm beam size để tăng tốc\n",
        "                language=\"vi\" if \"vietnamese\" in file_path.lower() else None  # Auto-detect hoặc specify\n",
        "            )\n",
        "\n",
        "            # Convert segments to list và extract words\n",
        "            whisper_words = []\n",
        "            for segment in segments:\n",
        "                for word in segment.words:\n",
        "                    whisper_words.append({\n",
        "                        'word': word.word,\n",
        "                        'start': word.start,\n",
        "                        'end': word.end\n",
        "                    })\n",
        "\n",
        "            if not whisper_words:\n",
        "                print(f\"{file_name} {file_type} - No words transcribed\")\n",
        "                with self._lock:\n",
        "                    self.cant_clean_list.append((file_name, file_type, \"no words transcribed\"))\n",
        "                return None, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{file_name} {file_type} - Can't transcribe: {e}\")\n",
        "            with self._lock:\n",
        "                self.cant_clean_list.append((file_name, file_type, \"cant transcribe\"))\n",
        "            return None, None\n",
        "\n",
        "        # Load reference text\n",
        "        try:\n",
        "            meta_path = os.path.join(speech_folder, file_name, f\"{file_name}.json\")\n",
        "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            if file_type == \"full\":\n",
        "                ref_text = data['command']\n",
        "            else:\n",
        "                num_seg = file_type.split(\"_\")[1]\n",
        "                ref_text = data['text_segments'][num_seg]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading reference text for {file_name}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            start, end = self.get_start_end_from_alignment(\n",
        "                ref_text=ref_text,\n",
        "                whisper_words=whisper_words\n",
        "            )\n",
        "\n",
        "            if padding:\n",
        "                start = max(0, start - padding)\n",
        "                end = end + padding\n",
        "\n",
        "            return start, end\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{file_name} {file_type} - Can't get start/end: {e}\")\n",
        "            with self._lock:\n",
        "                self.cant_clean_list.append((file_name, file_type, \"cant get start end\"))\n",
        "            return None, None\n",
        "\n",
        "    def trim_audio(self, file_name: str, file_type: str, start: float, end: float, speech_folder: str):\n",
        "        \"\"\"Tối ưu audio trimming\"\"\"\n",
        "        file_path = os.path.join(speech_folder, file_name, f\"{file_name}_{file_type}.wav\")\n",
        "\n",
        "        try:\n",
        "            audio = AudioSegment.from_wav(file_path)\n",
        "            if start is not None and end is not None:\n",
        "                # Convert to milliseconds\n",
        "                start_ms = int(start * 1000)\n",
        "                end_ms = int(end * 1000)\n",
        "                trimmed_audio = audio[start_ms:end_ms]\n",
        "                return trimmed_audio\n",
        "            else:\n",
        "                return audio\n",
        "        except Exception as e:\n",
        "            print(f\"Error trimming audio {file_name}_{file_type}: {e}\")\n",
        "            return AudioSegment.empty()\n",
        "\n",
        "    def get_start_end_from_alignment(self, ref_text: str, whisper_words: List[Dict]):\n",
        "        \"\"\"Tối ưu alignment algorithm\"\"\"\n",
        "        ref_words = self.normalize_text_cached(ref_text).split()\n",
        "        hypo_words = [self.normalize_text_cached(w['word']) for w in whisper_words]\n",
        "\n",
        "        if not ref_words or not hypo_words:\n",
        "            raise ValueError(\"Empty reference or hypothesis words\")\n",
        "\n",
        "        start_idx, end_idx = self.smith_waterman_fuzzy_optimized(ref_words, hypo_words)\n",
        "\n",
        "        if start_idx >= len(whisper_words) or end_idx >= len(whisper_words):\n",
        "            raise ValueError(\"Alignment indices out of bounds\")\n",
        "\n",
        "        start_time = whisper_words[start_idx]['start']\n",
        "        end_time = whisper_words[end_idx]['end']\n",
        "\n",
        "        return start_time, end_time\n",
        "\n",
        "    def smith_waterman_fuzzy_optimized(self, ref_words: List[str], hypo_words: List[str],\n",
        "                                     match_score=2, fuzzy_score=1, mismatch=-1, gap=-2):\n",
        "        \"\"\"Tối ưu Smith-Waterman algorithm\"\"\"\n",
        "        m, n = len(ref_words), len(hypo_words)\n",
        "\n",
        "        # Sử dụng numpy để tăng tốc\n",
        "        score = np.zeros((m+1, n+1), dtype=np.float32)\n",
        "        max_score = 0\n",
        "        max_pos = None\n",
        "\n",
        "        # Pre-compute similarities để tránh tính lại\n",
        "        similarities = np.zeros((m, n), dtype=np.float32)\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                similarities[i, j] = self.word_similarity_fast(ref_words[i], hypo_words[j])\n",
        "\n",
        "        for i in range(1, m+1):\n",
        "            for j in range(1, n+1):\n",
        "                sim = similarities[i-1, j-1]\n",
        "\n",
        "                if sim == 1:\n",
        "                    s = match_score\n",
        "                elif sim >= 0.8:\n",
        "                    s = fuzzy_score\n",
        "                else:\n",
        "                    s = mismatch\n",
        "\n",
        "                diag = score[i-1, j-1] + s\n",
        "                delete = score[i-1, j] + gap\n",
        "                insert = score[i, j-1] + gap\n",
        "                score[i, j] = max(0, diag, delete, insert)\n",
        "\n",
        "                if score[i, j] > max_score:\n",
        "                    max_score = score[i, j]\n",
        "                    max_pos = (i, j)\n",
        "\n",
        "        if max_pos is None:\n",
        "            raise ValueError(\"No alignment found\")\n",
        "\n",
        "        # Traceback\n",
        "        i, j = max_pos\n",
        "        end_j = j - 1\n",
        "\n",
        "        while i > 0 and j > 0 and score[i, j] > 0:\n",
        "            sim = similarities[i-1, j-1]\n",
        "            if sim >= 0.8:\n",
        "                i -= 1\n",
        "                j -= 1\n",
        "            elif score[i-1, j] + gap == score[i, j]:\n",
        "                i -= 1\n",
        "            else:\n",
        "                j -= 1\n",
        "\n",
        "        start_j = j\n",
        "        return start_j, end_j\n",
        "\n",
        "    @lru_cache(maxsize=50000)\n",
        "    def word_similarity_fast(self, w1: str, w2: str) -> float:\n",
        "        \"\"\"Cached version của word similarity\"\"\"\n",
        "        return SequenceMatcher(None, w1, w2).ratio()\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        if hasattr(self.model, 'model'):\n",
        "            del self.model.model\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    def get_performance_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get performance statistics\"\"\"\n",
        "        return {\n",
        "            \"device\": self.device,\n",
        "            # \"compute_type\": self.compute_type,\n",
        "            \"max_workers\": self.max_workers,\n",
        "            \"failed_files\": len(self.cant_clean_list),\n",
        "            \"gpu_available\": torch.cuda.is_available(),\n",
        "            \"gpu_memory_allocated\": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0,\n",
        "            \"gpu_memory_reserved\": torch.cuda.memory_reserved() if torch.cuda.is_available() else 0\n",
        "        }\n",
        "\n",
        "    # Backward compatibility methods\n",
        "    def str2list(self, list_str: str):\n",
        "        return ast.literal_eval(list_str)\n",
        "\n",
        "    def word_similarity(self, w1: str, w2: str) -> float:\n",
        "        return self.word_similarity_fast(w1, w2)\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        return self.normalize_text_cached(text)\n",
        "\n",
        "\n",
        "# Utility function để benchmark performance\n",
        "def benchmark_processing(cleaner: OptimizedSpeechCleaning, test_files: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"Benchmark processing performance\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for file_path in test_files:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Mock processing\n",
        "        try:\n",
        "            segments, info = cleaner.model.transcribe(file_path, word_timestamps=True)\n",
        "            processing_time = time.time() - start_time\n",
        "            results[file_path] = processing_time\n",
        "        except Exception as e:\n",
        "            results[file_path] = float('inf')\n",
        "            print(f\"Error benchmarking {file_path}: {e}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:22:29.02438Z",
          "iopub.execute_input": "2025-08-18T04:22:29.024891Z",
          "iopub.status.idle": "2025-08-18T04:22:29.05866Z",
          "shell.execute_reply.started": "2025-08-18T04:22:29.024869Z",
          "shell.execute_reply": "2025-08-18T04:22:29.057906Z"
        },
        "id": "BqyNICCNcdDw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Khởi tạo với các tối ưu\n",
        "    cleaner = OptimizedSpeechCleaning(\n",
        "        max_workers=8,\n",
        "        use_gpu=True,\n",
        "        model_size=\"medium\"  # hoặc \"large-v2\" cho accuracy cao hơn\n",
        "    )\n",
        "\n",
        "    # Sample data\n",
        "    df = pd.read_csv(\"/kaggle/working/100-samples.csv\", index_col = 0)\n",
        "\n",
        "    try:\n",
        "        # Process với monitoring\n",
        "        start_time = time.time()\n",
        "        cleaner.clean_pipeline(\n",
        "            export_dir = 'trimmed_speech',\n",
        "            speech_folder = '/kaggle/working/synthesis_command',\n",
        "            data = df, padding = 0.3,\n",
        "            batch_size = 4\n",
        "        )\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Total processing time: {total_time:.2f} seconds\")\n",
        "\n",
        "        # Print performance stats\n",
        "        stats = cleaner.get_performance_stats()\n",
        "        print(\"Performance Statistics:\")\n",
        "        for key, value in stats.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        cleaner.cleanup()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:22:29.904687Z",
          "iopub.execute_input": "2025-08-18T04:22:29.90495Z",
          "iopub.status.idle": "2025-08-18T04:26:20.289241Z",
          "shell.execute_reply.started": "2025-08-18T04:22:29.904934Z",
          "shell.execute_reply": "2025-08-18T04:26:20.288527Z"
        },
        "id": "aNkPPIw-cdDx",
        "outputId": "2c5419d4-4c22-4d4e-b698-afb07811ae6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using GPU: Tesla P100-PCIE-16GB\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "[2025-08-18 04:22:34.233] [ctranslate2] [thread 36] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Initialized with 8 workers on cuda\nProcessing batch 1/25\nsingle_mix_d337321c seg_0 - Can't get start/end: No alignment found\nProcessing batch 2/25\nchain_mix_79b56df5 full - No words transcribed\nsingle_mix_5143fc18 seg_3 - Can't get start/end: No alignment found\nsingle_mix_5143fc18 seg_9 - Can't get start/end: No alignment found\nsingle_mix_5143fc18 seg_11 - No words transcribed\nsingle_mix_5143fc18 seg_10 - Can't get start/end: No alignment found\nProcessing batch 3/25\nProcessing batch 4/25\nProcessing batch 5/25\nProcessing batch 6/25\nsingle_mix_966300b9 seg_0 - Can't get start/end: No alignment found\nProcessing batch 7/25\nProcessing batch 8/25\nProcessing batch 9/25\nProcessing batch 10/25\nProcessing batch 11/25\nsingle_active_7aa90624 full - Can't get start/end: No alignment found\nProcessing batch 12/25\nProcessing batch 13/25\nProcessing batch 14/25\nsingle_mix_c6252315 seg_0 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_1 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_2 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_4 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_3 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_5 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_7 - No words transcribed\nsingle_mix_c6252315 seg_8 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_9 - Can't get start/end: No alignment found\nsingle_mix_c6252315 seg_10 - Can't get start/end: No alignment found\nProcessing batch 15/25\nProcessing batch 16/25\nProcessing batch 17/25\nchain_mix_0969edce seg_0 - Can't get start/end: No alignment found\nProcessing batch 18/25\nchain_mix_460f79b4 seg_0 - Can't get start/end: No alignment found\nchain_mix_460f79b4 seg_1 - Can't get start/end: No alignment found\nProcessing batch 19/25\nchain_mix_b211eaf6 seg_0 - Can't get start/end: No alignment found\nchain_mix_b211eaf6 seg_2 - Can't get start/end: No alignment found\nsingle_mix_79892fde seg_2 - Can't get start/end: No alignment found\nProcessing batch 20/25\nsingle_active_6dc69584 full - Can't get start/end: No alignment found\nchain_mix_2465bf55 seg_0 - Can't get start/end: No alignment found\nProcessing batch 21/25\nsingle_active_c2a0338e full - Can't get start/end: No alignment found\nProcessing batch 22/25\nProcessing batch 23/25\nsingle_active_484e94d0 full - Can't get start/end: No alignment found\nProcessing batch 24/25\nchain_mix_974cfec8 seg_0 - Can't get start/end: No alignment found\nchain_mix_974cfec8 seg_2 - Can't get start/end: No alignment found\nProcessing batch 25/25\nProcessed 100 files in 225.67 seconds\nAverage: 2.26 seconds per file\nTotal processing time: 225.67 seconds\nPerformance Statistics:\n  device: cuda\n  max_workers: 8\n  failed_files: 30\n  gpu_available: True\n  gpu_memory_allocated: 0\n  gpu_memory_reserved: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!zip trimmed_speech.zip trimmed_speech -r"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:26:58.03159Z",
          "iopub.execute_input": "2025-08-18T04:26:58.031955Z",
          "iopub.status.idle": "2025-08-18T04:26:59.690095Z",
          "shell.execute_reply.started": "2025-08-18T04:26:58.031935Z",
          "shell.execute_reply": "2025-08-18T04:26:59.689366Z"
        },
        "id": "37WY90GpcdDx",
        "outputId": "a8a7c665-ae49-4f1d-80f6-cf2291afad69"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  adding: trimmed_speech/ (stored 0%)\n  adding: trimmed_speech/single_mix_acd9f593/ (stored 0%)\n  adding: trimmed_speech/single_mix_acd9f593/single_mix_acd9f593_full_trimmed.wav (deflated 32%)\n  adding: trimmed_speech/single_mix_acd9f593/single_mix_acd9f593_seg_0_trimmed.wav (deflated 23%)\n  adding: trimmed_speech/single_mix_acd9f593/single_mix_acd9f593_seg_1_trimmed.wav (deflated 17%)\n  adding: trimmed_speech/single_mix_640545be/ (stored 0%)\n  adding: trimmed_speech/single_mix_640545be/single_mix_640545be_seg_0_trimmed.wav (deflated 37%)\n  adding: trimmed_speech/single_mix_640545be/single_mix_640545be_full_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/single_mix_640545be/single_mix_640545be_seg_1_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_active_98293658/ (stored 0%)\n  adding: trimmed_speech/chain_active_98293658/chain_active_98293658_full_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/chain_mix_0969edce/ (stored 0%)\n  adding: trimmed_speech/chain_mix_0969edce/chain_mix_0969edce_seg_1_trimmed.wav (deflated 17%)\n  adding: trimmed_speech/chain_mix_0969edce/chain_mix_0969edce_seg_2_trimmed.wav (deflated 28%)\n  adding: trimmed_speech/chain_mix_0969edce/chain_mix_0969edce_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/single_mix_cdfe23ab/ (stored 0%)\n  adding: trimmed_speech/single_mix_cdfe23ab/single_mix_cdfe23ab_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_cdfe23ab/single_mix_cdfe23ab_full_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_cdfe23ab/single_mix_cdfe23ab_seg_0_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/single_active_ae9c20f6/ (stored 0%)\n  adding: trimmed_speech/single_active_ae9c20f6/single_active_ae9c20f6_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/non_active_d725ea6b/ (stored 0%)\n  adding: trimmed_speech/non_active_d725ea6b/non_active_d725ea6b_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_1ac7590b/ (stored 0%)\n  adding: trimmed_speech/chain_mix_1ac7590b/chain_mix_1ac7590b_seg_0_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_1ac7590b/chain_mix_1ac7590b_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_1ac7590b/chain_mix_1ac7590b_seg_1_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/single_active_146d6eed/ (stored 0%)\n  adding: trimmed_speech/single_active_146d6eed/single_active_146d6eed_full_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/non_active_30e9739d/ (stored 0%)\n  adding: trimmed_speech/non_active_30e9739d/non_active_30e9739d_full_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/single_mix_cdeb248e/ (stored 0%)\n  adding: trimmed_speech/single_mix_cdeb248e/single_mix_cdeb248e_seg_1_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/single_mix_cdeb248e/single_mix_cdeb248e_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/single_mix_cdeb248e/single_mix_cdeb248e_seg_0_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/non_active_a08a84f7/ (stored 0%)\n  adding: trimmed_speech/non_active_a08a84f7/non_active_a08a84f7_full_trimmed.wav (deflated 31%)\n  adding: trimmed_speech/non_active_5e8907d2/ (stored 0%)\n  adding: trimmed_speech/non_active_5e8907d2/non_active_5e8907d2_full_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/non_active_ff07bd62/ (stored 0%)\n  adding: trimmed_speech/non_active_ff07bd62/non_active_ff07bd62_full_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/chain_mix_372f7c04/ (stored 0%)\n  adding: trimmed_speech/chain_mix_372f7c04/chain_mix_372f7c04_seg_0_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_372f7c04/chain_mix_372f7c04_full_trimmed.wav (deflated 25%)\n  adding: trimmed_speech/chain_mix_372f7c04/chain_mix_372f7c04_seg_2_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_372f7c04/chain_mix_372f7c04_seg_1_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_active_f3ea6e3b/ (stored 0%)\n  adding: trimmed_speech/single_active_f3ea6e3b/single_active_f3ea6e3b_full_trimmed.wav (deflated 32%)\n  adding: trimmed_speech/non_active_5ef41e47/ (stored 0%)\n  adding: trimmed_speech/non_active_5ef41e47/non_active_5ef41e47_full_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/single_mix_a0e38162/ (stored 0%)\n  adding: trimmed_speech/single_mix_a0e38162/single_mix_a0e38162_seg_0_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_a0e38162/single_mix_a0e38162_full_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_a0e38162/single_mix_a0e38162_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/chain_mix_2465bf55/ (stored 0%)\n  adding: trimmed_speech/chain_mix_2465bf55/chain_mix_2465bf55_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_2465bf55/chain_mix_2465bf55_seg_2_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_2465bf55/chain_mix_2465bf55_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/chain_mix_460f79b4/ (stored 0%)\n  adding: trimmed_speech/chain_mix_460f79b4/chain_mix_460f79b4_seg_2_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_460f79b4/chain_mix_460f79b4_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/single_mix_966300b9/ (stored 0%)\n  adding: trimmed_speech/single_mix_966300b9/single_mix_966300b9_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/single_mix_966300b9/single_mix_966300b9_seg_1_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/non_active_0c8efe14/ (stored 0%)\n  adding: trimmed_speech/non_active_0c8efe14/non_active_0c8efe14_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_b21f4dee/ (stored 0%)\n  adding: trimmed_speech/chain_mix_b21f4dee/chain_mix_b21f4dee_seg_0_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/chain_mix_b21f4dee/chain_mix_b21f4dee_full_trimmed.wav (deflated 32%)\n  adding: trimmed_speech/chain_mix_b21f4dee/chain_mix_b21f4dee_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_2e9958c3/ (stored 0%)\n  adding: trimmed_speech/single_mix_2e9958c3/single_mix_2e9958c3_seg_1_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/single_mix_2e9958c3/single_mix_2e9958c3_seg_0_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_mix_2e9958c3/single_mix_2e9958c3_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/single_mix_c6252315/ (stored 0%)\n  adding: trimmed_speech/single_mix_c6252315/single_mix_c6252315_seg_6_trimmed.wav (deflated 25%)\n  adding: trimmed_speech/single_mix_c6252315/single_mix_c6252315_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/single_active_16cdf505/ (stored 0%)\n  adding: trimmed_speech/single_active_16cdf505/single_active_16cdf505_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/single_mix_c0c0d41d/ (stored 0%)\n  adding: trimmed_speech/single_mix_c0c0d41d/single_mix_c0c0d41d_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_mix_c0c0d41d/single_mix_c0c0d41d_seg_0_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/single_mix_c0c0d41d/single_mix_c0c0d41d_seg_1_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_mix_4021432a/ (stored 0%)\n  adding: trimmed_speech/single_mix_4021432a/single_mix_4021432a_seg_1_trimmed.wav (deflated 6%)\n  adding: trimmed_speech/single_mix_4021432a/single_mix_4021432a_seg_0_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/single_mix_4021432a/single_mix_4021432a_full_trimmed.wav (deflated 6%)\n  adding: trimmed_speech/non_active_87a7d469/ (stored 0%)\n  adding: trimmed_speech/non_active_87a7d469/non_active_87a7d469_full_trimmed.wav (deflated 23%)\n  adding: trimmed_speech/single_mix_5143fc18/ (stored 0%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_2_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_8_trimmed.wav (deflated 19%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_6_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_1_trimmed.wav (deflated 20%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_7_trimmed.wav (deflated 48%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_full_trimmed.wav (deflated 25%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_0_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_4_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/single_mix_5143fc18/single_mix_5143fc18_seg_5_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_active_b2c8a897/ (stored 0%)\n  adding: trimmed_speech/single_active_b2c8a897/single_active_b2c8a897_full_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/single_mix_445f45d6/ (stored 0%)\n  adding: trimmed_speech/single_mix_445f45d6/single_mix_445f45d6_seg_0_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/single_mix_445f45d6/single_mix_445f45d6_full_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/single_mix_445f45d6/single_mix_445f45d6_seg_1_trimmed.wav (deflated 21%)\n  adding: trimmed_speech/single_mix_71aa1602/ (stored 0%)\n  adding: trimmed_speech/single_mix_71aa1602/single_mix_71aa1602_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_mix_71aa1602/single_mix_71aa1602_seg_0_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/single_mix_71aa1602/single_mix_71aa1602_seg_1_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/non_active_2523ceb0/ (stored 0%)\n  adding: trimmed_speech/non_active_2523ceb0/non_active_2523ceb0_full_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/chain_mix_76c08e2c/ (stored 0%)\n  adding: trimmed_speech/chain_mix_76c08e2c/chain_mix_76c08e2c_seg_1_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/chain_mix_76c08e2c/chain_mix_76c08e2c_full_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/chain_mix_76c08e2c/chain_mix_76c08e2c_seg_0_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/chain_mix_8410bf34/ (stored 0%)\n  adding: trimmed_speech/chain_mix_8410bf34/chain_mix_8410bf34_full_trimmed.wav (deflated 6%)\n  adding: trimmed_speech/chain_mix_8410bf34/chain_mix_8410bf34_seg_1_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/chain_mix_8410bf34/chain_mix_8410bf34_seg_2_trimmed.wav (deflated 4%)\n  adding: trimmed_speech/chain_mix_8410bf34/chain_mix_8410bf34_seg_0_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/chain_mix_fa4ee976/ (stored 0%)\n  adding: trimmed_speech/chain_mix_fa4ee976/chain_mix_fa4ee976_full_trimmed.wav (deflated 28%)\n  adding: trimmed_speech/chain_mix_fa4ee976/chain_mix_fa4ee976_seg_1_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_fa4ee976/chain_mix_fa4ee976_seg_0_trimmed.wav (deflated 38%)\n  adding: trimmed_speech/chain_mix_fa4ee976/chain_mix_fa4ee976_seg_3_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/chain_mix_fa4ee976/chain_mix_fa4ee976_seg_2_trimmed.wav (deflated 46%)\n  adding: trimmed_speech/chain_active_86db051f/ (stored 0%)\n  adding: trimmed_speech/chain_active_86db051f/chain_active_86db051f_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/non_active_977bade5/ (stored 0%)\n  adding: trimmed_speech/non_active_977bade5/non_active_977bade5_full_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/chain_active_d49388c3/ (stored 0%)\n  adding: trimmed_speech/chain_active_d49388c3/chain_active_d49388c3_full_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/chain_active_37c0c753/ (stored 0%)\n  adding: trimmed_speech/chain_active_37c0c753/chain_active_37c0c753_full_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/non_active_aae314c5/ (stored 0%)\n  adding: trimmed_speech/non_active_aae314c5/non_active_aae314c5_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_79b56df5/ (stored 0%)\n  adding: trimmed_speech/chain_mix_79b56df5/chain_mix_79b56df5_seg_2_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_79b56df5/chain_mix_79b56df5_seg_1_trimmed.wav (deflated 21%)\n  adding: trimmed_speech/chain_mix_79b56df5/chain_mix_79b56df5_seg_0_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/chain_active_de3cf381/ (stored 0%)\n  adding: trimmed_speech/chain_active_de3cf381/chain_active_de3cf381_full_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/single_active_fc43ec93/ (stored 0%)\n  adding: trimmed_speech/single_active_fc43ec93/single_active_fc43ec93_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_active_19837594/ (stored 0%)\n  adding: trimmed_speech/chain_active_19837594/chain_active_19837594_full_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/non_active_d32124ed/ (stored 0%)\n  adding: trimmed_speech/non_active_d32124ed/non_active_d32124ed_full_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/chain_active_2821b1e6/ (stored 0%)\n  adding: trimmed_speech/chain_active_2821b1e6/chain_active_2821b1e6_full_trimmed.wav (deflated 31%)\n  adding: trimmed_speech/chain_active_481a2089/ (stored 0%)\n  adding: trimmed_speech/chain_active_481a2089/chain_active_481a2089_full_trimmed.wav (deflated 23%)\n  adding: trimmed_speech/chain_mix_410ea272/ (stored 0%)\n  adding: trimmed_speech/chain_mix_410ea272/chain_mix_410ea272_seg_2_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/chain_mix_410ea272/chain_mix_410ea272_seg_1_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/chain_mix_410ea272/chain_mix_410ea272_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/chain_mix_410ea272/chain_mix_410ea272_seg_0_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_active_49a6cb5a/ (stored 0%)\n  adding: trimmed_speech/single_active_49a6cb5a/single_active_49a6cb5a_full_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_active_5122bba3/ (stored 0%)\n  adding: trimmed_speech/single_active_5122bba3/single_active_5122bba3_full_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/chain_active_d6424fdf/ (stored 0%)\n  adding: trimmed_speech/chain_active_d6424fdf/chain_active_d6424fdf_full_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/non_active_b3eb8308/ (stored 0%)\n  adding: trimmed_speech/non_active_b3eb8308/non_active_b3eb8308_full_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/single_mix_bb5351d0/ (stored 0%)\n  adding: trimmed_speech/single_mix_bb5351d0/single_mix_bb5351d0_seg_1_trimmed.wav (deflated 3%)\n  adding: trimmed_speech/single_mix_bb5351d0/single_mix_bb5351d0_seg_0_trimmed.wav (deflated 6%)\n  adding: trimmed_speech/single_mix_bb5351d0/single_mix_bb5351d0_full_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/non_active_fcbf2e1f/ (stored 0%)\n  adding: trimmed_speech/non_active_fcbf2e1f/non_active_fcbf2e1f_full_trimmed.wav (deflated 17%)\n  adding: trimmed_speech/chain_active_75ec7c53/ (stored 0%)\n  adding: trimmed_speech/chain_active_75ec7c53/chain_active_75ec7c53_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_ce374f32/ (stored 0%)\n  adding: trimmed_speech/chain_mix_ce374f32/chain_mix_ce374f32_seg_0_trimmed.wav (deflated 35%)\n  adding: trimmed_speech/chain_mix_ce374f32/chain_mix_ce374f32_seg_2_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/chain_mix_ce374f32/chain_mix_ce374f32_seg_1_trimmed.wav (deflated 30%)\n  adding: trimmed_speech/chain_mix_ce374f32/chain_mix_ce374f32_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_mix_d337321c/ (stored 0%)\n  adding: trimmed_speech/single_mix_d337321c/single_mix_d337321c_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/single_mix_d337321c/single_mix_d337321c_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_d337321c/single_mix_d337321c_seg_2_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_5a93f120/ (stored 0%)\n  adding: trimmed_speech/chain_mix_5a93f120/chain_mix_5a93f120_seg_1_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_5a93f120/chain_mix_5a93f120_seg_0_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/chain_mix_5a93f120/chain_mix_5a93f120_full_trimmed.wav (deflated 23%)\n  adding: trimmed_speech/chain_mix_5a93f120/chain_mix_5a93f120_seg_2_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/chain_active_86aee56b/ (stored 0%)\n  adding: trimmed_speech/chain_active_86aee56b/chain_active_86aee56b_full_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_active_602bd3d7/ (stored 0%)\n  adding: trimmed_speech/single_active_602bd3d7/single_active_602bd3d7_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_active_ea845303/ (stored 0%)\n  adding: trimmed_speech/chain_active_ea845303/chain_active_ea845303_full_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/single_mix_4ad2be32/ (stored 0%)\n  adding: trimmed_speech/single_mix_4ad2be32/single_mix_4ad2be32_seg_1_trimmed.wav (deflated 19%)\n  adding: trimmed_speech/single_mix_4ad2be32/single_mix_4ad2be32_full_trimmed.wav (deflated 22%)\n  adding: trimmed_speech/single_mix_4ad2be32/single_mix_4ad2be32_seg_0_trimmed.wav (deflated 17%)\n  adding: trimmed_speech/single_active_6c10fb58/ (stored 0%)\n  adding: trimmed_speech/single_active_6c10fb58/single_active_6c10fb58_full_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/single_active_4ff7f40d/ (stored 0%)\n  adding: trimmed_speech/single_active_4ff7f40d/single_active_4ff7f40d_full_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/chain_mix_f3fc5238/ (stored 0%)\n  adding: trimmed_speech/chain_mix_f3fc5238/chain_mix_f3fc5238_seg_0_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_f3fc5238/chain_mix_f3fc5238_full_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/chain_mix_f3fc5238/chain_mix_f3fc5238_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/chain_mix_f3fc5238/chain_mix_f3fc5238_seg_2_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_0e75cf2d/ (stored 0%)\n  adding: trimmed_speech/chain_mix_0e75cf2d/chain_mix_0e75cf2d_seg_2_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/chain_mix_0e75cf2d/chain_mix_0e75cf2d_seg_1_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_0e75cf2d/chain_mix_0e75cf2d_seg_0_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_mix_0e75cf2d/chain_mix_0e75cf2d_full_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_active_070e3895/ (stored 0%)\n  adding: trimmed_speech/single_active_070e3895/single_active_070e3895_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/single_active_71e5194f/ (stored 0%)\n  adding: trimmed_speech/single_active_71e5194f/single_active_71e5194f_full_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/single_active_dd52e4d7/ (stored 0%)\n  adding: trimmed_speech/single_active_dd52e4d7/single_active_dd52e4d7_full_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/non_active_b7fda903/ (stored 0%)\n  adding: trimmed_speech/non_active_b7fda903/non_active_b7fda903_full_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/chain_active_4f27fa8c/ (stored 0%)\n  adding: trimmed_speech/chain_active_4f27fa8c/chain_active_4f27fa8c_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/single_active_55ca3e19/ (stored 0%)\n  adding: trimmed_speech/single_active_55ca3e19/single_active_55ca3e19_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_active_a7a6afdc/ (stored 0%)\n  adding: trimmed_speech/chain_active_a7a6afdc/chain_active_a7a6afdc_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/non_active_02b82303/ (stored 0%)\n  adding: trimmed_speech/non_active_02b82303/non_active_02b82303_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_mix_bbfc7792/ (stored 0%)\n  adding: trimmed_speech/single_mix_bbfc7792/single_mix_bbfc7792_seg_1_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/single_mix_bbfc7792/single_mix_bbfc7792_full_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_bbfc7792/single_mix_bbfc7792_seg_0_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/chain_mix_793a310d/ (stored 0%)\n  adding: trimmed_speech/chain_mix_793a310d/chain_mix_793a310d_seg_2_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_793a310d/chain_mix_793a310d_full_trimmed.wav (deflated 30%)\n  adding: trimmed_speech/chain_mix_793a310d/chain_mix_793a310d_seg_1_trimmed.wav (deflated 17%)\n  adding: trimmed_speech/chain_mix_793a310d/chain_mix_793a310d_seg_0_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/chain_mix_793a310d/chain_mix_793a310d_seg_3_trimmed.wav (deflated 6%)\n  adding: trimmed_speech/chain_mix_b211eaf6/ (stored 0%)\n  adding: trimmed_speech/chain_mix_b211eaf6/chain_mix_b211eaf6_full_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/chain_mix_b211eaf6/chain_mix_b211eaf6_seg_1_trimmed.wav (deflated 6%)\n  adding: trimmed_speech/non_active_48b2f5f9/ (stored 0%)\n  adding: trimmed_speech/non_active_48b2f5f9/non_active_48b2f5f9_full_trimmed.wav (deflated 25%)\n  adding: trimmed_speech/chain_active_af7b2342/ (stored 0%)\n  adding: trimmed_speech/chain_active_af7b2342/chain_active_af7b2342_full_trimmed.wav (deflated 25%)\n  adding: trimmed_speech/non_active_c2299169/ (stored 0%)\n  adding: trimmed_speech/non_active_c2299169/non_active_c2299169_full_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/chain_mix_b31f1fb7/ (stored 0%)\n  adding: trimmed_speech/chain_mix_b31f1fb7/chain_mix_b31f1fb7_seg_3_trimmed.wav (deflated 15%)\n  adding: trimmed_speech/chain_mix_b31f1fb7/chain_mix_b31f1fb7_seg_2_trimmed.wav (deflated 26%)\n  adding: trimmed_speech/chain_mix_b31f1fb7/chain_mix_b31f1fb7_seg_0_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/chain_mix_b31f1fb7/chain_mix_b31f1fb7_seg_1_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/chain_mix_b31f1fb7/chain_mix_b31f1fb7_full_trimmed.wav (deflated 29%)\n  adding: trimmed_speech/non_active_33793242/ (stored 0%)\n  adding: trimmed_speech/non_active_33793242/non_active_33793242_full_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/chain_mix_974cfec8/ (stored 0%)\n  adding: trimmed_speech/chain_mix_974cfec8/chain_mix_974cfec8_full_trimmed.wav (deflated 13%)\n  adding: trimmed_speech/chain_mix_974cfec8/chain_mix_974cfec8_seg_1_trimmed.wav (deflated 11%)\n  adding: trimmed_speech/non_active_06fd3645/ (stored 0%)\n  adding: trimmed_speech/non_active_06fd3645/non_active_06fd3645_full_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/single_mix_aaaa437e/ (stored 0%)\n  adding: trimmed_speech/single_mix_aaaa437e/single_mix_aaaa437e_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_mix_aaaa437e/single_mix_aaaa437e_seg_1_trimmed.wav (deflated 5%)\n  adding: trimmed_speech/single_mix_aaaa437e/single_mix_aaaa437e_seg_0_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_active_b1d24a75/ (stored 0%)\n  adding: trimmed_speech/chain_active_b1d24a75/chain_active_b1d24a75_full_trimmed.wav (deflated 17%)\n  adding: trimmed_speech/chain_active_5fa2e8c9/ (stored 0%)\n  adding: trimmed_speech/chain_active_5fa2e8c9/chain_active_5fa2e8c9_full_trimmed.wav (deflated 10%)\n  adding: trimmed_speech/chain_active_776af78e/ (stored 0%)\n  adding: trimmed_speech/chain_active_776af78e/chain_active_776af78e_full_trimmed.wav (deflated 8%)\n  adding: trimmed_speech/single_mix_79892fde/ (stored 0%)\n  adding: trimmed_speech/single_mix_79892fde/single_mix_79892fde_seg_0_trimmed.wav (deflated 31%)\n  adding: trimmed_speech/single_mix_79892fde/single_mix_79892fde_full_trimmed.wav (deflated 14%)\n  adding: trimmed_speech/single_mix_79892fde/single_mix_79892fde_seg_3_trimmed.wav (deflated 21%)\n  adding: trimmed_speech/single_mix_79892fde/single_mix_79892fde_seg_1_trimmed.wav (deflated 9%)\n  adding: trimmed_speech/single_mix_5abb52e2/ (stored 0%)\n  adding: trimmed_speech/single_mix_5abb52e2/single_mix_5abb52e2_seg_1_trimmed.wav (deflated 20%)\n  adding: trimmed_speech/single_mix_5abb52e2/single_mix_5abb52e2_seg_0_trimmed.wav (deflated 12%)\n  adding: trimmed_speech/single_mix_5abb52e2/single_mix_5abb52e2_full_trimmed.wav (deflated 24%)\n  adding: trimmed_speech/chain_active_584a79e1/ (stored 0%)\n  adding: trimmed_speech/chain_active_584a79e1/chain_active_584a79e1_full_trimmed.wav (deflated 7%)\n  adding: trimmed_speech/single_active_d08929af/ (stored 0%)\n  adding: trimmed_speech/single_active_d08929af/single_active_d08929af_full_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/chain_mix_61c6f9ad/ (stored 0%)\n  adding: trimmed_speech/chain_mix_61c6f9ad/chain_mix_61c6f9ad_seg_0_trimmed.wav (deflated 16%)\n  adding: trimmed_speech/chain_mix_61c6f9ad/chain_mix_61c6f9ad_seg_2_trimmed.wav (deflated 21%)\n  adding: trimmed_speech/chain_mix_61c6f9ad/chain_mix_61c6f9ad_seg_1_trimmed.wav (deflated 35%)\n  adding: trimmed_speech/chain_mix_61c6f9ad/chain_mix_61c6f9ad_full_trimmed.wav (deflated 18%)\n  adding: trimmed_speech/chain_active_0d63eaae/ (stored 0%)\n  adding: trimmed_speech/chain_active_0d63eaae/chain_active_0d63eaae_full_trimmed.wav (deflated 9%)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import itertools\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class SpeechCleaning:\n",
        "    def __init__(self, cant_clean_list = None):\n",
        "        self.model = whisper.load_model(\"medium\")\n",
        "        if not cant_clean_list:\n",
        "            self.cant_clean_list = []\n",
        "        else:\n",
        "            self.cant_clean_list = cant_clean_list\n",
        "    def clean_pipeline(self, export_dir: str, data: pd.DataFrame, speech_folder: str, padding = None):\n",
        "        os.makedirs(export_dir, exist_ok = True)\n",
        "        for idx, row in data.iterrows():\n",
        "\n",
        "            file_name = f\"{row['type']}_{row['id']}\"\n",
        "            file_type = \"full\"\n",
        "\n",
        "            start, end = self.get_clean_range(file_name = file_name, file_type = file_type, padding = padding, speech_folder = speech_folder)\n",
        "            trimmed_audio = self.trim_audio(file_name = file_name, file_type = file_type,\n",
        "                                                      start = start, end = end, speech_folder = speech_folder)\n",
        "\n",
        "            os.makedirs(os.path.join(export_dir, file_name), exist_ok = True)\n",
        "            out_path = os.path.join(export_dir, file_name, f\"{file_name}_{file_type}_trimmed.wav\")\n",
        "            trimmed_audio.export(out_path, format=\"wav\")\n",
        "\n",
        "            if row['type'] in ['single_mix', 'chain_mix']:\n",
        "\n",
        "               for i in range(row['num_segments']):\n",
        "                   file_type = f\"seg_{i}\"\n",
        "                   start, end = self.get_clean_range(file_name = file_name, file_type = file_type, padding = padding, speech_folder = speech_folder)\n",
        "                   trimmed_audio = self.trim_audio(file_name = file_name, file_type = file_type,\n",
        "                                                              start = start, end = end, speech_folder = speech_folder)\n",
        "\n",
        "                   os.makedirs(os.path.join(export_dir, file_name), exist_ok = True)\n",
        "                   out_path = os.path.join(export_dir, file_name, f\"{file_name}_{file_type}_trimmed.wav\")\n",
        "                   trimmed_audio.export(out_path, format=\"wav\")\n",
        "\n",
        "\n",
        "    def get_clean_range(self, file_name: str, file_type: str, speech_folder: str, padding = None):\n",
        "        # prepare whisper words\n",
        "        file_path = os.path.join(speech_folder, file_name, f\"{file_name}_{file_type}.wav\")\n",
        "\n",
        "        transcribe = self.model.transcribe(file_path, word_timestamps=True)\n",
        "        try:\n",
        "            whisper_words = [speaker['words'] for speaker in transcribe['segments']]\n",
        "\n",
        "            whisper_words = list(itertools.chain.from_iterable(whisper_words))\n",
        "        except Exception as e:\n",
        "            print(file_name + \" \" + file_type + \" \" + 'cant transcribe')\n",
        "            self.cant_clean_list.append((file_name, file_type, \"cant transcribe\"))\n",
        "            return None, None\n",
        "\n",
        "        # prepare reference text\n",
        "        meta_path = os.path.join(speech_folder, file_name, f\"{file_name}.json\")\n",
        "\n",
        "        with open(meta_path, \"r\", encoding = \"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if file_type == \"full\":\n",
        "            ref_text = data['command']\n",
        "        else:\n",
        "            num_seg = file_type.split(\"_\")[1]\n",
        "            ref_text = data['text_segments'][num_seg]\n",
        "\n",
        "        try:\n",
        "            start, end = self.get_start_end_from_alignment(ref_text = ref_text,\n",
        "                                                  whisper_words = whisper_words)\n",
        "            if padding:\n",
        "                start = max(0, start - padding)\n",
        "                end = end + padding\n",
        "        except Exception as e:\n",
        "            # Problems with long synthesis, should use only with segment\n",
        "            print(file_name + \" \" + file_type + \" \" + 'cant get start end')\n",
        "            print(e)\n",
        "            print(f'ref_text: {ref_text}')\n",
        "            print(f'whisper_words: {whisper_words}')\n",
        "            self.cant_clean_list.append((file_name, file_type, \"cant get start end\"))\n",
        "            return None, None\n",
        "        return start, end\n",
        "\n",
        "    def trim_audio(self, file_name, file_type, start, end, speech_folder):\n",
        "        file_path = os.path.join(speech_folder, file_name, f\"{file_name}_{file_type}.wav\")\n",
        "\n",
        "        audio = AudioSegment.from_wav(file_path)\n",
        "        if start and end:\n",
        "\n",
        "            trimmed_audio = audio[start*1000 : end*1000]\n",
        "            return trimmed_audio\n",
        "        else:\n",
        "            return audio\n",
        "\n",
        "    def get_start_end_from_alignment(self, ref_text, whisper_words):\n",
        "        ref_words = self.normalize_text(ref_text).split()\n",
        "        hypo_words = [self.normalize_text(w['word']) for w in whisper_words]\n",
        "\n",
        "        start_idx, end_idx = self.smith_waterman_fuzzy(ref_words, hypo_words)\n",
        "        start_time = whisper_words[start_idx]['start']\n",
        "        end_time = whisper_words[end_idx]['end']\n",
        "\n",
        "        return start_time, end_time\n",
        "\n",
        "    def smith_waterman_fuzzy(self, ref_words, hypo_words, match_score=2, fuzzy_score=1, mismatch=-1, gap=-2):\n",
        "        m, n = len(ref_words), len(hypo_words)\n",
        "        score = np.zeros((m+1, n+1))\n",
        "        max_score = 0\n",
        "        max_pos = None\n",
        "\n",
        "        for i in range(1, m+1):\n",
        "            for j in range(1, n+1):\n",
        "                sim = self.word_similarity(ref_words[i-1], hypo_words[j-1])\n",
        "                if sim == 1:\n",
        "                    s = match_score\n",
        "                elif sim >= 0.8:\n",
        "                    s = fuzzy_score\n",
        "                else:\n",
        "                    s = mismatch\n",
        "\n",
        "                diag = score[i-1, j-1] + s\n",
        "                delete = score[i-1, j] + gap\n",
        "                insert = score[i, j-1] + gap\n",
        "                score[i, j] = max(0, diag, delete, insert)\n",
        "\n",
        "                if score[i, j] > max_score:\n",
        "                    max_score = score[i, j]\n",
        "                    max_pos = (i, j)\n",
        "\n",
        "        # Traceback\n",
        "        i, j = max_pos\n",
        "        end_j = j - 1\n",
        "        while i > 0 and j > 0 and score[i, j] > 0:\n",
        "            sim = self.word_similarity(ref_words[i-1], hypo_words[j-1])\n",
        "            if sim >= 0.8:\n",
        "                i -= 1\n",
        "                j -= 1\n",
        "            elif score[i-1, j] + gap == score[i, j]:\n",
        "                i -= 1\n",
        "            else:\n",
        "                j -= 1\n",
        "        start_j = j\n",
        "\n",
        "        return start_j, end_j\n",
        "\n",
        "    def str2list(self, list_str: str):\n",
        "        return ast.literal_eval(list_str)\n",
        "\n",
        "    def word_similarity(self, w1, w2):\n",
        "        return SequenceMatcher(None, w1, w2).ratio()\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text.lower()).strip()\n",
        "\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T03:33:58.996199Z",
          "iopub.execute_input": "2025-08-18T03:33:58.996926Z",
          "iopub.status.idle": "2025-08-18T03:33:59.015516Z",
          "shell.execute_reply.started": "2025-08-18T03:33:58.996902Z",
          "shell.execute_reply": "2025-08-18T03:33:59.014793Z"
        },
        "id": "OCD-EUcNcdDy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SpeechCleaning()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T03:34:00.240053Z",
          "iopub.execute_input": "2025-08-18T03:34:00.240376Z",
          "iopub.status.idle": "2025-08-18T03:34:13.696788Z",
          "shell.execute_reply.started": "2025-08-18T03:34:00.240353Z",
          "shell.execute_reply": "2025-08-18T03:34:13.696263Z"
        },
        "id": "dC4LmrxMcdDy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/working/100-samples.csv\", index_col = 0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T03:34:13.698657Z",
          "iopub.execute_input": "2025-08-18T03:34:13.698864Z",
          "iopub.status.idle": "2025-08-18T03:34:13.706273Z",
          "shell.execute_reply.started": "2025-08-18T03:34:13.698848Z",
          "shell.execute_reply": "2025-08-18T03:34:13.705489Z"
        },
        "id": "EArtACHJcdDz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sc.clean_pipeline(export_dir = 'trimmed_speech', speech_folder = '/kaggle/working/synthesis_command', data = df, padding = 0.3)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T03:34:13.707171Z",
          "iopub.execute_input": "2025-08-18T03:34:13.707448Z",
          "iopub.status.idle": "2025-08-18T03:41:20.378974Z",
          "shell.execute_reply.started": "2025-08-18T03:34:13.707422Z",
          "shell.execute_reply": "2025-08-18T03:41:20.378139Z"
        },
        "id": "9xys32BBcdDz",
        "outputId": "f4a19355-8e0a-47e0-f90d-1ab83883bfd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "single_active_4ff7f40d full cant get start end\ncannot unpack non-iterable NoneType object\nref_text: show me Cartoon Network\nwhisper_words: [{'word': ' What', 'start': 0.44000000000000006, 'end': 0.88, 'probability': 0.2911490797996521}, {'word': ' a', 'start': 0.88, 'end': 1.08, 'probability': 0.19421282410621643}, {'word': ' pleasant', 'start': 1.08, 'end': 1.3, 'probability': 0.23018227517604828}, {'word': ' tome,', 'start': 1.3, 'end': 1.7, 'probability': 0.5129103735089302}, {'word': \" you're\", 'start': 1.76, 'end': 1.94, 'probability': 0.4445706903934479}, {'word': ' a', 'start': 1.94, 'end': 2.0, 'probability': 0.19106259942054749}, {'word': ' first', 'start': 2.0, 'end': 2.26, 'probability': 0.5872543454170227}, {'word': '-gen', 'start': 2.26, 'end': 2.72, 'probability': 0.4452144652605057}, {'word': ' dab,', 'start': 2.72, 'end': 3.14, 'probability': 0.5577821135520935}, {'word': ' and', 'start': 3.46, 'end': 3.64, 'probability': 0.920242965221405}, {'word': ' here', 'start': 3.64, 'end': 3.8, 'probability': 0.814730703830719}, {'word': ' that', 'start': 3.8, 'end': 4.0, 'probability': 0.8039153814315796}, {'word': ' day', 'start': 4.0, 'end': 4.24, 'probability': 0.909503698348999}, {'word': ' is', 'start': 4.24, 'end': 4.4, 'probability': 0.6538426280021667}, {'word': ' uberbeck', 'start': 4.4, 'end': 4.92, 'probability': 0.5682819336652756}, {'word': ' frady.', 'start': 4.92, 'end': 5.16, 'probability': 0.5877803415060043}]\nsingle_mix_d337321c seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: Can you\nwhisper_words: []\nchain_mix_79b56df5 full cant get start end\ncannot unpack non-iterable NoneType object\nref_text: set volume to 10, do you remember my birthday is next week? switch to channel Cartoon Network.\nwhisper_words: []\nsingle_mix_5143fc18 seg_1 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: while\nwhisper_words: [{'word': ' Amazingly', 'start': 0.0, 'end': 0.0, 'probability': 0.005326938109647017}, {'word': ' cool.', 'start': 0.0, 'end': 1.14, 'probability': 0.006055118050426245}, {'word': ' Yes', 'start': 1.16, 'end': 1.56, 'probability': 0.004368339199572802}, {'word': ' sir,', 'start': 1.56, 'end': 2.1, 'probability': 0.003025027457624674}, {'word': ' The', 'start': 2.12, 'end': 3.2, 'probability': 0.0013099685311317444}, {'word': ' you', 'start': 3.18, 'end': 3.2, 'probability': 0.07016493380069733}]\nsingle_mix_5143fc18 seg_3 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: you\nwhisper_words: [{'word': ' JSONU', 'start': 0.0, 'end': 1.9, 'probability': 0.2021235004067421}]\nsingle_mix_5143fc18 seg_5 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: she\nwhisper_words: [{'word': ' Jazshii', 'start': 0.0, 'end': 1.9, 'probability': 0.12103683159997065}]\nsingle_mix_5143fc18 seg_9 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: at\nwhisper_words: []\nsingle_mix_5143fc18 seg_10 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: the\nwhisper_words: [{'word': ' JSON', 'start': 0.0, 'end': 1.78, 'probability': 0.529983639717102}]\nsingle_mix_966300b9 seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: set volume to 10\nwhisper_words: [{'word': ' Jason', 'start': 0.56, 'end': 1.26, 'probability': 0.2885092496871948}, {'word': ' You', 'start': 1.26, 'end': 1.5, 'probability': 0.0038972662296146154}]\nsingle_active_7aa90624 full cant get start end\ncannot unpack non-iterable NoneType object\nref_text: turn off subtitles\nwhisper_words: [{'word': ' Adorno,', 'start': 0.5599999999999998, 'end': 0.96, 'probability': 0.4713745787739754}, {'word': ' subtype.', 'start': 1.1, 'end': 1.36, 'probability': 0.5043217614293098}]\nsingle_mix_c6252315 seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: What\nwhisper_words: [{'word': ' Jason?', 'start': 0.0, 'end': 1.1, 'probability': 0.7641662359237671}]\nsingle_mix_c6252315 seg_1 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: do\nwhisper_words: [{'word': ' Jason', 'start': 0.0, 'end': 0.54, 'probability': 0.04797322303056717}]\nsingle_mix_c6252315 seg_2 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: you\nwhisper_words: [{'word': \" She's\", 'start': 0.0, 'end': 1.0, 'probability': 0.5927341431379318}, {'word': ' on.', 'start': 1.0, 'end': 1.0, 'probability': 0.4240727126598358}]\nsingle_mix_c6252315 seg_3 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: think\nwhisper_words: [{'word': \" I'm\", 'start': 0.0, 'end': 0.8, 'probability': 0.28169900365173817}, {'word': ' true', 'start': 0.8, 'end': 1.2, 'probability': 0.00026478510699234903}, {'word': ' Jen', 'start': 1.2, 'end': 1.64, 'probability': 0.05564477667212486}]\nsingle_mix_c6252315 seg_4 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: about\nwhisper_words: [{'word': ' This', 'start': 0.0, 'end': 0.98, 'probability': 0.5200225114822388}, {'word': ' one?', 'start': 0.98, 'end': 1.2, 'probability': 0.9363523125648499}, {'word': ' VOTE!', 'start': 1.58, 'end': 1.7, 'probability': 0.6384379267692566}]\nsingle_mix_c6252315 seg_5 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: that\nwhisper_words: [{'word': ' Jessen', 'start': 0.0, 'end': 1.36, 'probability': 0.5439496785402298}]\nsingle_mix_c6252315 seg_7 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: Set\nwhisper_words: [{'word': \" That's\", 'start': 0.0, 'end': 0.9, 'probability': 0.5607360526919365}, {'word': ' them.', 'start': 0.9, 'end': 1.0, 'probability': 0.34214717149734497}]\nsingle_mix_c6252315 seg_8 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: brightness\nwhisper_words: [{'word': ' Jeff', 'start': 0.3799999999999999, 'end': 0.74, 'probability': 0.7997738718986511}, {'word': ' Rice?', 'start': 0.74, 'end': 0.92, 'probability': 0.11012589186429977}]\nsingle_mix_c6252315 seg_9 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: to\nwhisper_words: [{'word': ' Jason', 'start': 0.0, 'end': 0.6, 'probability': 0.2780616283416748}]\nsingle_mix_c6252315 seg_10 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: 70.\nwhisper_words: []\nchain_mix_0969edce seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: unmute the TV\nwhisper_words: [{'word': ' Jason?', 'start': 0.0, 'end': 0.84, 'probability': 0.7884073257446289}]\nchain_mix_460f79b4 seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: scroll down\nwhisper_words: [{'word': ' Just', 'start': 0.9199999999999997, 'end': 1.2, 'probability': 0.3735354542732239}, {'word': ' go', 'start': 1.2, 'end': 1.34, 'probability': 0.7086660861968994}, {'word': ' on.', 'start': 1.34, 'end': 1.38, 'probability': 0.931724488735199}]\nchain_mix_460f79b4 seg_1 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: skip intro\nwhisper_words: [{'word': '以上', 'start': 0.0, 'end': 0.98, 'probability': 0.289216548204422}]\nsingle_mix_79892fde seg_2 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: while\nwhisper_words: [{'word': ' JSON,', 'start': 0.74, 'end': 1.26, 'probability': 0.651055634021759}, {'word': ' wild.', 'start': 1.66, 'end': 1.78, 'probability': 0.6291753053665161}]\nchain_mix_b211eaf6 seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: Set the volume to minimum\nwhisper_words: [{'word': ' below', 'start': 0.33999999999999997, 'end': 0.66, 'probability': 0.0736817717552185}, {'word': ' is', 'start': 0.66, 'end': 0.78, 'probability': 0.264555424451828}, {'word': ' costumes,', 'start': 0.78, 'end': 0.92, 'probability': 3.267126885475591e-05}, {'word': ' Jason.', 'start': 1.24, 'end': 1.4, 'probability': 0.4045279920101166}]\nchain_mix_b211eaf6 seg_2 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: rewind that show\nwhisper_words: [{'word': ' Jason.', 'start': 0.0, 'end': 0.02, 'probability': 0.7733314037322998}]\nsingle_active_6dc69584 full cant get start end\ncannot unpack non-iterable NoneType object\nref_text: open Spotify\nwhisper_words: [{'word': ' Jason.', 'start': 0.0, 'end': 0.68, 'probability': 0.5289405584335327}]\nchain_mix_2465bf55 seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: show me Nickelodeon\nwhisper_words: [{'word': ' Jason.', 'start': 0.0, 'end': 1.06, 'probability': 0.7531425356864929}]\nsingle_active_c2a0338e full cant get start end\ncannot unpack non-iterable NoneType object\nref_text: close Spotify\nwhisper_words: [{'word': ' Jess,', 'start': 0.0, 'end': 0.92, 'probability': 0.056910011917352676}, {'word': \" I'm\", 'start': 1.0, 'end': 1.0, 'probability': 0.9068235158920288}]\nsingle_active_484e94d0 full cant get start end\ncannot unpack non-iterable NoneType object\nref_text: mute the TV\nwhisper_words: [{'word': ' Jen', 'start': 0.0, 'end': 0.74, 'probability': 0.26775455474853516}]\nchain_mix_974cfec8 seg_0 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: change input to Netflix\nwhisper_words: [{'word': \" You've\", 'start': 0.45999999999999996, 'end': 0.76, 'probability': 0.4204769432544708}, {'word': ' got', 'start': 0.76, 'end': 0.88, 'probability': 0.8438436388969421}, {'word': ' a', 'start': 0.88, 'end': 0.96, 'probability': 0.6837365031242371}, {'word': ' whole', 'start': 0.96, 'end': 1.1, 'probability': 0.12884150445461273}, {'word': ' different', 'start': 1.1, 'end': 1.3, 'probability': 0.07384108006954193}, {'word': ' chain', 'start': 1.3, 'end': 1.46, 'probability': 0.1608496606349945}]\nchain_mix_974cfec8 seg_2 cant get start end\ncannot unpack non-iterable NoneType object\nref_text: turn off subtitles\nwhisper_words: [{'word': ' Tell', 'start': 0.28, 'end': 0.52, 'probability': 0.4803788363933563}, {'word': ' me', 'start': 0.52, 'end': 0.66, 'probability': 0.9434592127799988}, {'word': ' when', 'start': 0.66, 'end': 0.7, 'probability': 0.26091980934143066}, {'word': \" I'm\", 'start': 0.7, 'end': 0.82, 'probability': 0.5700608789920807}, {'word': ' not', 'start': 0.82, 'end': 0.9, 'probability': 0.5968596935272217}, {'word': ' subscribed.', 'start': 0.9, 'end': 1.12, 'probability': 0.45113837718963623}]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sc.cant_clean_list"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T03:41:40.282426Z",
          "iopub.execute_input": "2025-08-18T03:41:40.282732Z",
          "iopub.status.idle": "2025-08-18T03:41:40.289051Z",
          "shell.execute_reply.started": "2025-08-18T03:41:40.282711Z",
          "shell.execute_reply": "2025-08-18T03:41:40.28848Z"
        },
        "id": "_4kDAyd3cdDz",
        "outputId": "be3b66c9-316a-48e2-a76d-d1571baa1dad"
      },
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[('single_active_4ff7f40d', 'full', 'cant get start end'),\n ('single_mix_d337321c', 'seg_0', 'cant get start end'),\n ('chain_mix_79b56df5', 'full', 'cant get start end'),\n ('single_mix_5143fc18', 'seg_1', 'cant get start end'),\n ('single_mix_5143fc18', 'seg_3', 'cant get start end'),\n ('single_mix_5143fc18', 'seg_5', 'cant get start end'),\n ('single_mix_5143fc18', 'seg_9', 'cant get start end'),\n ('single_mix_5143fc18', 'seg_10', 'cant get start end'),\n ('single_mix_966300b9', 'seg_0', 'cant get start end'),\n ('single_active_7aa90624', 'full', 'cant get start end'),\n ('single_mix_c6252315', 'seg_0', 'cant get start end'),\n ('single_mix_c6252315', 'seg_1', 'cant get start end'),\n ('single_mix_c6252315', 'seg_2', 'cant get start end'),\n ('single_mix_c6252315', 'seg_3', 'cant get start end'),\n ('single_mix_c6252315', 'seg_4', 'cant get start end'),\n ('single_mix_c6252315', 'seg_5', 'cant get start end'),\n ('single_mix_c6252315', 'seg_7', 'cant get start end'),\n ('single_mix_c6252315', 'seg_8', 'cant get start end'),\n ('single_mix_c6252315', 'seg_9', 'cant get start end'),\n ('single_mix_c6252315', 'seg_10', 'cant get start end'),\n ('chain_mix_0969edce', 'seg_0', 'cant get start end'),\n ('chain_mix_460f79b4', 'seg_0', 'cant get start end'),\n ('chain_mix_460f79b4', 'seg_1', 'cant get start end'),\n ('single_mix_79892fde', 'seg_2', 'cant get start end'),\n ('chain_mix_b211eaf6', 'seg_0', 'cant get start end'),\n ('chain_mix_b211eaf6', 'seg_2', 'cant get start end'),\n ('single_active_6dc69584', 'full', 'cant get start end'),\n ('chain_mix_2465bf55', 'seg_0', 'cant get start end'),\n ('single_active_c2a0338e', 'full', 'cant get start end'),\n ('single_active_484e94d0', 'full', 'cant get start end'),\n ('chain_mix_974cfec8', 'seg_0', 'cant get start end'),\n ('chain_mix_974cfec8', 'seg_2', 'cant get start end')]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "a-aCAwRXcdD0"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}